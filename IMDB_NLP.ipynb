{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB_NLP","provenance":[],"collapsed_sections":[],"mount_file_id":"1T-s7itX2JLGfPAB9v95QlSVosLxeDwRL","authorship_tag":"ABX9TyMTBWyoUJ3/avxn+U7y/ZHY"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# change here to your dataset path\n","# PATH = '/content/drive/MyDrive/imdb_nlp/IMDB Dataset.csv'\n","PATH = 'IMDB Dataset.csv'"]},{"source":["## **1. Background**"],"cell_type":"markdown","metadata":{}},{"source":["![Natural language processing](https://landbot.io/wp-content/uploads/2019/11/natural-language-processing-chatbot.jpg)"],"cell_type":"markdown","metadata":{}},{"source":["**What is Natural Language Processing?**\n","\n","From wikipedia, Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n","\n","**What is Sentiment Classification?**\n","\n","Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n","\n","**What is Tokenizer?**\n","\n","Tokenization is a necessary first step in many natural language processing tasks, such as word counting, parsing, spell checking, corpus generation, and statistical analysis of text.\n","\n","Tokenizer is a compact pure-Python (2 and 3) executable program and module for tokenizing Icelandic text. It converts input text to streams of tokens, where each token is a separate word, punctuation sign, number/amount, date, e-mail, URL/URI, etc. It also segments the token stream into sentences, considering corner cases such as abbreviations and dates in the middle of sentences.[Tokenizer](https://pypi.org/project/tokenizer/)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Serfati/imdb_sentiment_analysis)"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !pip install -r requirements.txt\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"]},{"cell_type":"code","metadata":{"id":"87-_quCtzm_4","executionInfo":{"status":"ok","timestamp":1607427985912,"user_tz":-120,"elapsed":999,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}}},"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import os\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualization\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('ggplot')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import homogeneity_score\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix"]},{"source":["## **2. Data exploratory analysis**\n"],"cell_type":"markdown","metadata":{}},{"source":["### **2.1 Data overview**\n","\n","![IMDB 50 review datasets](https://o.aolcdn.com/images/dims?quality=85&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdims%3Fcrop%3D908%252C537%252C0%252C0%26quality%3D85%26format%3Djpg%26resize%3D1600%252C947%26image_uri%3Dhttps%253A%252F%252Fs.yimg.com%252Fos%252Fcreatr-uploaded-images%252F2019-08%252F560e5d20-c833-11e9-bf26-36635805fe83%26client%3Da1acac3e1b3290917d92%26signature%3D639a4965c41ca6cec13652498f65cfc97170ea5d&client=amp-blogside-v2&signature=765e155477177a69b93eac5611145d4241be6071)"],"cell_type":"markdown","metadata":{}},{"source":["This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. This document outlines how the dataset was gathered, and how to use the files provided.\n","\n","**Dataset**\n","\n","The core dataset contains 50,000 reviews. The overall distribution of labels is balanced (25k pos and 25k neg). We also include an additional 50,000 unlabeled documents for unsupervised learning."],"cell_type":"markdown","metadata":{}},{"source":["### **2.2 Data Exploration**"],"cell_type":"markdown","metadata":{}},{"source":["The first step is to load the data to global environment."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"TigiU-nyzn4K","executionInfo":{"status":"ok","timestamp":1607427987241,"user_tz":-120,"elapsed":2298,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}}},"source":["df = pd.read_csv(PATH)"],"execution_count":null,"outputs":[]},{"source":["We could see some abnormal words such as <br /><br />, then we should replace them by a null or space value."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure()\n","plt.hist(df['review'].str.split().apply(len).value_counts())\n","plt.xlabel('number of words in sentence')\n","plt.ylabel('frequency')\n","plt.title('Words occurrence frequency')"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"d0yRw7Y0znwB","executionInfo":{"status":"ok","timestamp":1607427987242,"user_tz":-120,"elapsed":2291,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"011076b9-f79f-4e85-850e-079e30c64382"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_kYH3y6znkh","executionInfo":{"status":"ok","timestamp":1607427987243,"user_tz":-120,"elapsed":2281,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"b8b757fb-0b38-41d5-bc11-ad8abed9daf1"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"SwZy4AK-z6DZ","executionInfo":{"status":"ok","timestamp":1607427987244,"user_tz":-120,"elapsed":2271,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"3639c6ae-953f-4ab8-952c-71b1b44930b6"},"source":["df[\"review\"][0][:250]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vly3acnwz6A3","executionInfo":{"status":"ok","timestamp":1607427987245,"user_tz":-120,"elapsed":2261,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"45f98ecb-d45f-4310-a4fd-bbf5d8a60e4d"},"source":["df[\"sentiment\"][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJQyEYcjz5-f","executionInfo":{"status":"ok","timestamp":1607427987246,"user_tz":-120,"elapsed":2251,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"5d532f85-a106-4cb5-ca07-e6907f32be01"},"source":["df[\"review\"].groupby(df[\"sentiment\"]).count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"opMhAMQdz573","executionInfo":{"status":"ok","timestamp":1607427987247,"user_tz":-120,"elapsed":2240,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"90ac9f37-16aa-44db-dbe2-9228234e429b"},"source":["s = df['sentiment'].value_counts()\n","s = (s/s.sum())*100\n","\n","plt.figure()\n","bars = plt.bar(s.index, s.values, color = ['green', 'red'], alpha = .6)\n","plt.xticks(s.index, ['Positive', 'Negative'], fontsize = 15)\n","plt.tick_params(bottom = False, top = False, left = False, right = False, labelleft = False)\n","for spine in plt.gca().spines.values():\n","    spine.set_visible(False)\n","for bar in bars:\n","    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 5, s = str(bar.get_height())[:2] + '%', ha = 'center', fontsize = 15)\n","plt.title('Reviews polarity', fontsize = 17)"],"execution_count":null,"outputs":[]},{"source":["### **2.3 Data pre-processing**"],"cell_type":"markdown","metadata":{}},{"cell_type":"markdown","metadata":{"id":"HoJs-9B_0fa1"},"source":["**Text Cleaning**\n","\n","0.Label Encoder\n","\n","1.Remove html tags\n","\n","2.Remove special characters\n","\n","3.Converting every thing to lower case\n","\n","4.Removing Stop words\n","\n","5.Stemming\n","\n","6.Remove extra spaces\n","\n","7.Lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_encoder = LabelEncoder()\n","df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n","df.head()"]},{"cell_type":"code","metadata":{"id":"0_y3JL040qZl","executionInfo":{"status":"ok","timestamp":1607427987622,"user_tz":-120,"elapsed":2597,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}}},"source":["df['review'] = df['review'].str.replace('<br />','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# function to remove special characters\n","df[\"review\"] = df[\"review\"].apply(lambda x: re.sub(\"[^0-9a-zA-Z]\",' ', x))"]},{"cell_type":"code","metadata":{"id":"QvPFHlqf0qKO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607428030068,"user_tz":-120,"elapsed":45036,"user":{"displayName":"Avihai Serfati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvO4rMcdiqMGGA3D_i3M8xS3N3qZIpAW2WKnTB=s64","userId":"05143496911826234602"}},"outputId":"a5d52405-b75a-4056-a7bb-b58e0ce5ed40"},"source":["df['review'] = df['review'].str.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words(\"english\"))"]},{"source":["**Stop Words Removal**\n","\n","We'll remove the stop words for better prediction."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","pbar = tqdm(total=df.shape[0], leave=True, position=0)\n","\n","def remove_sw(review):\n","    tokens = word_tokenize(review)\n","    tokens = [w for w in tokens if not w in stop_words]\n","    pbar.update(1)\n","    return \" \".join(tokens)\n","\n","df[\"review\"] = df[\"review\"].apply(remove_sw)\n","pbar.close()"]},{"source":["**Porter Stemmer**\n","\n","For this particular dataset the PorterStemmer does not bring better performance, so it is better to skip this step."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"FQS-rhtY0qEp"},"source":["pbar = tqdm(total=df.shape[0], leave=True, position=0)\n","ps = PorterStemmer()\n","\n","def stem(text):\n","    pbar.update(1)\n","    return ' '.join([ps.stem(word) for word in text.split()])\n","\n","df[\"review\"] = df[\"review\"].apply(stem)\n","pbar.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('punkt')\n","#Creating a Lemmatizer for preprocessing\n","class LemmaTokenizer:\n","    def __init__(self):\n","        self.wnl = WordNetLemmatizer()\n","    def __call__(self, doc):\n","        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"review\"] = df[\"review\"].apply(lambda x: re.sub(\" +\",\" \", x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# A cleaned review after pre proccessing\n","df['review'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["neg = df[df['sentiment'] == 0]\n","pos = df[df['sentiment'] == 1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.express as px\n","def get_top_text_ngrams(corpus, n, g):\n","    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n","    bag_of_words = vec.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","    return words_freq[:n]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["most_common_uni = get_top_text_ngrams(neg.review,10,1)\n","most_common_uni = dict(most_common_uni)\n","temp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\n","temp[\"Common_words\"] = list(most_common_uni.keys())\n","temp[\"Count\"] = list(most_common_uni.values())\n","fig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Unigram - Commmon Words in Negative Text', orientation='h', \n","             width=700, height=700,color='Common_words')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["most_common_uni = get_top_text_ngrams(pos.review,10,1)\n","most_common_uni = dict(most_common_uni)\n","temp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\n","temp[\"Common_words\"] = list(most_common_uni.keys())\n","temp[\"Count\"] = list(most_common_uni.values())\n","fig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Unigram - Commmon Words in Positive Text', orientation='h', \n","             width=700, height=700,color='Common_words')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["most_common_uni = get_top_text_ngrams(pos.review,10,2)\n","most_common_uni = dict(most_common_uni)\n","temp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\n","temp[\"Common_words\"] = list(most_common_uni.keys())\n","temp[\"Count\"] = list(most_common_uni.values())\n","fig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Bigram - Commmon Words in Positive Text', orientation='h', \n","             width=700, height=700,color='Common_words')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["most_common_uni = get_top_text_ngrams(pos.review,10,2)\n","most_common_uni = dict(most_common_uni)\n","temp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\n","temp[\"Common_words\"] = list(most_common_uni.keys())\n","temp[\"Count\"] = list(most_common_uni.values())\n","fig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Bigram - Commmon Words in Positive Text', orientation='h', \n","             width=700, height=700,color='Common_words')\n","fig.show()"]},{"cell_type":"code","metadata":{"id":"dXDprN1i6OlJ"},"source":["X = df['review']\n","y = df['sentiment']"],"execution_count":null,"outputs":[]},{"source":["Split data to train and test for modeling and performance evaluation."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"kIeTkIFa1Byt"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)\n","\n","print('Training dataset : {} reviews'.format(X_train.shape[0]))\n","print('Testing dataset : {} reviews'.format(X_test.shape[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_test.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"aT4YKKBjAuhm"},"source":["## **3. Modeling**"]},{"source":["### 3.1 Feature Extraction using TF-IDF algorithm\n","\n","![TFIDF](https://miro.medium.com/max/532/0*bHkPdhgfnyTs4un_)\n","\n","In scikit-learn, the TF-IDF algorithm is implemented using **TfidfTransformer**. This transformer needs the count matrix which it will transform later. Hence, we use **CountVectorizer** first.\n","Alternatively, one can use **TfidfVectorizer**, which is the equivalent of CountVectorizer followed by TfidfTransformer"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"03QUaef_1BsZ"},"source":["tfidfVect =    TfidfVectorizer( max_df = 0.5,\n","                                sublinear_tf=True,\n","                                lowercase = True, \n","                                ngram_range = (1,2), \n","                                tokenizer = LemmaTokenizer(),\n","                                stop_words = 'english',\n","                                min_df = 1,\n","                                use_idf = True,\n","                                # max_features = 1000,\n","                                strip_accents = 'ascii'\n","                                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%time features = tfidfVect.fit_transform(X_train)\n","features.shape"]},{"source":["## Unsupervised Learning Approach\n","\n","Now, all that’s left to do is use a machine learning algorithm. We can summarize all that we have done so far using a scikit-learn pipeline."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"BgiEX8EJ1Bmh"},"source":["model = KMeans(n_clusters=2, random_state=42)\n","\n","#fit the model with data (occurs in-place)\n","model.fit(features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsa2RRxP2JPg"},"source":["features_test = tfidfVect.transform(X_test)"],"execution_count":null,"outputs":[]},{"source":["We can find predictions using the predict() method."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"XorKmDw_2d8v"},"source":["pred = model.predict(features_test)\n","pred = pd.DataFrame(pred)"],"execution_count":null,"outputs":[]},{"source":["To evaluate the predictions, we use different classification metrics."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"test accuracy score: {0:.3f}%\".format(accuracy_score(y_test, pred)*100))\n","#accuracy_score ==> 74.352%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sn.heatmap(confusion_matrix(y_test, pred), annot=True, cmap=\"icefire\",xticklabels=['Negative', 'Positive'],yticklabels=\n","['Negative', 'Positive'], fmt='g')\n","\n","sn.color_palette(\"pastel\")\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('KMeans Confusion Matrix')"]},{"source":["We have obtained than 74.4% accuracy in predicting whether the review message is positive or negative, and we have performed feature extraction from the raw text in the process."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"Fcz-dFjU1BYy"},"source":["# pca = PCA(n_components=2)\n","# reduced_features = pca.fit_transform(features.toarray()[:1000])\n","# reduced_cluster_centers = pca.transform(model.cluster_centers_)\n","\n","# plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=model.predict(features), s=10)\n","# plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:, 1], marker='x', s=150, c='b')"],"execution_count":null,"outputs":[]},{"source":["## Supervised Learning Approach"],"cell_type":"markdown","metadata":{}},{"source":["## TfIdfVectorizer Feature Extraction \n","**Naive Bayes Classifier for Multinomial**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#instantiate the model (with the default parameters)\n","mnb = MultinomialNB()\n","\n","#fit the model with data (occurs in-place)\n","mnb.fit(features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of MultinomialNB using TfIdfVectorizer: {0:.3f}%\".format(accuracy_score(y_train, mnb.predict(features))*100))\n","#Training accuracy of MultinomialNB using TfIdfVectorizer: 98.848%"]},{"source":["**Stochastic Gradient Descent Classifier**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import SGDClassifier\n","#instantiate the model (with the default parameters)\n","sgd = SGDClassifier()\n","\n","#fit the model with data (occurs in-place)\n","sgd.fit(features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of SGDClassifier using TfIdfVectorizer: {0:.3f}%\".format(accuracy_score(y_train, rfc.predict(features))*100))"]},{"source":["**Logistic Regression**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","#instantiate the model (with the default parameters)\n","lr = LogisticRegression()\n","\n","#fit the model with data (occurs in-place)\n","lr.fit(features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of Logistic Regression using TfIdfVectorizer: {0:.3f}%\".format(accuracy_score(y_train, lr.predict(features))*100))\n","#Training accuracy of Logistic Regression using TfIdfVectorizer: 96.088%"]},{"source":["### 3.1 Feature Extraction using Count Vectorizer"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv=CountVectorizer( max_df = 0.5,\n","                    lowercase = True, \n","                    ngram_range = (1,2), \n","                    tokenizer = LemmaTokenizer(),\n","                    stop_words = 'english',\n","                    min_df = 1,\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%time bow_features = cv.fit_transform(X_train)\n","bow_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%time bow_features_test = bow.transform(X_test)"]},{"source":["**MultinomialNB**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#instantiate the model (with the default parameters)\n","mnb2 = MultinomialNB()\n","\n","#fit the model with data (occurs in-place)\n","mnb2.fit(bow_features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of MultinomialNB using CountVectorizer: {0:.3f}%\".format(accuracy_score(y_train, mnb2.predict(bow_features))*100))"]},{"source":["**LogisticRegression**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#instantiate the model (with the default parameters)\n","lr2 = LogisticRegression()\n","\n","#fit the model with data (occurs in-place)\n","lr2.fit(bow_features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of LogisticRegression using CountVectorizer: {0:.3f}%\".format(accuracy_score(y_train, lr2.predict(bow_features))*100))"]},{"source":["**SGDClassifier**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#instantiate the model (with the default parameters)\n","sgd2 = SGDClassifier()\n","\n","#fit the model with data (occurs in-place)\n","sgd2.fit(bow_features, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Training accuracy of SGDClassifier using CountVectorizer: {0:.3f}%\".format(accuracy_score(y_train, sgd2.predict(bow_features))*100))"]},{"source":["## Evaluation"],"cell_type":"markdown","metadata":{}},{"source":["We'll take the model with the highest training accuracy score and evaluate the test set with it. \n","In out case the **MultinomialNB** model provied us the highest score."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features_test = tfidfVect.transform(X_test)\n","pred = mnb.predict(features_test)\n","pred = pd.DataFrame(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Test accuracy score of MultinomialNB using TfIdfVectorizer: {0:.3f}%\".format(accuracy_score(y_test, pred)*100))\n","# accuracy score: 83.516%\n","print(\"Test AUC score of MultinomialNB using TfIdfVectorizer: {0:.3f} %\".format(roc_auc_score(y_test, pred)*100))"]}]}